---
layout: page
title: Introduction to Hadoop
---

> ## Learning objectives {.objectives}
>
> By the end of this workshop, you will be able to
>
> 1. Create, manage and navigate files and directories on
>    the Hadoop Distributed File System (HDFS).
> 2. Understand the concept of data locality in distributed system.
> 3. Understand how big data files are distributed across HDFS and manipulated by MapReduce programming paradigm to facilitate data locality.
> 4. Write Streaming MapReduce Python programs to analyze a large data set.
> 5. Integrate Streaming MapReduce processing with standard Python programs to facilitate complex analysis.

> ## Prerequesites {.prereq}
> This workshop requires prerequisites knowledges that are equivalent to the following COE workshops:
>
> 1. Introduction to research computing on the Palmetto Cluster.
> 2. Introduction to Linux.
> 3. Introduction to Python.

## Contributing

These lessons are modeled after the structure of
[Data Carpentry][dc-lessons] lesson materials,
an open source project.
Like Data Carpentry, we welcome contributions
of all kinds:
new lessons,
fixes/improvements to existing material,
corrections to typos,
bug reports,
and reviews of proposed changes are all equally welcome.
Please see our page on [Contributing][contributing]
to get started.

## Introduction to Hadoop

1. [Introduction to the Hadoop Distributed File System (HDFS)](00-intro.html)
2. [Interaction with the Hadoop cluster](01-interaction.html)
3. [Files and Directories](02-filedir.html)
4. [MapReduce Programming Paradigm](03-mapreduce.html)
5. [Running a Streaming MapReduce program](04-streaming-mapreduce.html)
6. [Creating a Mapper](05-mapper.html)
7. [Creating a Reducer](06-reducer.html)
8. [Integrating MapReduce](07-integrating-mapreduce.html)

[dc-lessons]: https://datacarpentry.org/lessons/
[contributing]: https://github.com/shwina/hpc-novice/blob/gh-pages/CONTRIBUTING.md
